

本人5年工作经验， 熟悉Hadoop体系及各种组件, 熟悉MapReduce的工作原理，熟悉数据仓库建模及ETL设计开发，
熟练使用Hive进行海量数据的清洗、加工、查询和统计分析及调优，熟悉报表开发及可视化

有3年以上经验，有带领工作团队经验，积极向上，有良好的人际沟通能力，
良好的工作协调能力，踏实肯干的工作精神，不断学习新技术，对知识有强烈的求知欲，
良好的前端编程能力和编程习惯，致力于代码的整体结构规范及优化。




1.负责数据仓库建模，相关生产系统数据的抽取、清洗、转换、落地等工作
2.负责风险指标预警项目指标体系构建，数据计算及报表产出
3.负责内审报表系统开发、计算与维护


1.负责针对业务情况进行专题数据分析，并进行归类形成报表体系
2.负责报表的可视化
3.负责临时数据需求的完成


工作内容：
1、负责针对业务情况进行专题数据分析，输出解决方案，跟进业务优化，以数据驱动业务增长；
2、建立平台销售、毛利润、退款率、ROI监控数据体系，及时发现异常，提出数据建议；
3、协助业务部门获取日常经营数据，挖掘业务部门的真实需求，并进行分析归类形成报表体系；
4、深入了解业务，根据业务发展需要，搭建和优化数据体系，有效解释数据异常；
5、快速响应异常数据分析需求。

.协助完成系统日常运营数据跟踪、分析及业务指标落实情况跟踪等


参与项目中数据仓库的构建和维护
充分理解业务需求，不断完善的数据体系，建设稳定、高效的数据仓库
北京创亿盛鑫网络科技服务有限公司

1、熟练使用Linux常用的操作命令, 了解Shell脚本编程
2、熟悉Hadoop体系架构各种组件, 熟悉MapReduce的工作原理
3、熟悉HDFS架构, Yarn架构以及编写MapRduce程序, 熟悉Java语言
4、熟悉Hive架构, 能用Hive进行海量数据的统计分析和查询, 熟悉Hive的调优
5、熟练大数据数仓建模, 熟练进行业务开发
6、熟练Flume和Sqoop技术。能用Flume做到高效的数据采集, 用Sqoop在Hadoop和关系型数据库之间传递数据
7、熟悉Spark体系, 熟悉SparkSQL
8、了解常用的机器学习算法, 例如逻辑回归, 线性回归, 决策树, 随机森林等
9、熟悉阿里云大数据平台Maxcompute, DataWorks, PyOdps等套件
10、熟练使用Python语言, 了解Numpy和Pandas, 可以进行简单的爬虫(Selenium, Scrapy)
11、熟练使用Quick BI, Tableau等工具进行可视化


熟悉Hadoop体系架构各种组件, 熟悉MapReduce的工作原理


项目描述：对于采集的日志和业务数据进行,整合分析:对数据进行过滤, 分层、统计各项指标提取有价值的信
1. 进行分层:实现原始数据的隔离,减少重复开发, 以及复杂问题的简单化对数据进行清洗过滤:去除脏数据：
脱敏.维度退化等•主要分为5层QDS层QWD层,DWS层QWT层,ADS层
2. ODS层:保留原始数据不做修改:对数据进行压缩:减少磁盘空间;使用分区表:避免:全盘扫描
3. DWD层,数据清洗:对手机号, 身份证号地址等进行脱敏操作、对日志数据进行解析:采用维度建模:进行维
度退化和创建事实表.采用LZO压缩和列式存储数据.
4. DWS层, 统计个主题的当日行为,服务于DWT层的主题竞表.
5. DWT层、以分析的主题对象为建模驱动：ODS层的应用和产品指标需求构建主题对象的全类竞表
6. ODS层对电商各大主题分别进行分析
7,将ADS层分析出的数据导入MySQL进行可视化展示
&使用Azkaban数据仓库各层的脚本进行调度
9,使用PrestoXylin进行即席查询使用Atlas管理元数据Range「进行权限管理
10,使用Hive分析工具对数据仓库进行分析                   https://blog.csdn.net/qq_26442553


用户注册、贷款申请、风控审核、放款、贷后还款、催收等, 
这些业务环节的事务会在不同的系统完成；催收有催收系统, 
贷款申请有CRM系统等；这些围绕业务主线, 涉及用户, 内部员工
, 三方机构从而产生的业务数据, 行为数据等都会通过每天定时或者实时存入数仓系统。


基于联通各地区短信数据进行分析和挖掘：
1.联通短信数据日常离线分析：
将短信从黑指纹库（黑指纹数据库为Mysql）导入到
hadoop集群中(使用Sqoop工具),使用Spark分析每日新
增垃圾短信特征, 从而调整算法, 和增加新特征, 从而划分人工核优选级。
2.月度分析、年度分析、各地区分析（分钟实时）：
能过Saprk_rdd与Hadoop_mr分析每月、每年短信的变化, 如指纹的月使用率, 年使用率, 各地区黑指纹通用率, 垃圾短信的变种率等
我的职责
1.利用sqoop工具对mysql数据库的增量抽取到HDFS
2.利用hive整合把得到的有效数据进行导入导出
3.将数据映射到hive中进行归类、划分、合并、定性分析、对关键字的分
析任务等
4.在Hive中根据业务需求参与相关表设计
5.对HDFS、Hive等进行调优, HQL语句进行编写、调优
6.根据业务的要求使用spark进行分钟实时分析
7.将数据的分析结果备份到HDFS中和业务层的数据库中（mysql）
8.web端或者报表的的形式进行展示。




项目名称 : 交互式数据分析系统（准实时数据分析平台）
技术架构 : Hadoop+Flume+Kafka+Sqoop+Spark+Zookeeper+JDBC+Hive+Mysql+Highcharts+FreeMarker
开发环境 : Linux+Window+IDEA+Maven+Tomcat +JDK+SVN(版本管理工具)
项目描述 : 该系统是一个交互式用户行为分析系统。系统的主要用户为公司内部的PM和运营人员, 用户根据自己的需求去
分析某一 类客户的流量数据。根据分析结果, PM可以优化产品设计, 运营人员可以为自己的运营工作提供数据
支持。用户在系统界面中选择某个分析功能对应的菜单, 并进入对应的任务创建界面, 然后选择筛选条件和任务
参数, 并提交任务。在接收到用户提交的任务之后, 根据任务类型选择其对应的Spark作业, 启动一条子线
程来执行Spark-submit命令以提交Spark作业。Spark作业运行在Yarn集群上, 并针对Hdfs中的海量
数据使用SparkSQL进行计算, 最终将计算结果写入Hdfs中。另外还集成Flume, Kafka和Spark,
利用SparkStreaming, 进行实时分析。用户通过系统界面查看任务分析结果, 将结果返回给界面进行展现。
项目职责 :负责项目的集群搭建部署, 需求分析, 参与库表的设计及代码的编写和测试优化工作
开发步骤 : 数据收集阶段：Flume采集供应商接口上的数据,实现flume自定义拦截器,满足业务需求。
Kafka拉取flume上的数据传给sparkStreaming进行数据处理（实时ETL的程序）, ETL处理的数据保存到Hdfs中。
数据处理阶段：SparkCore处理Hdfs上的数据, 通过RDD进行离线处理和Dstream准实时处理之后保存到Hdfs中。
数据展示阶段：通过JDBC连接Hdfs处理好的数据, 进行图表展示。
功能模块 : 用户访问会话（session）分析模块, 页面跳出率统计分析模块, 
各区域热门商品统计分析模块, 广告点击流量实时统计分析模块




能够安装、部署、Hadoop2.x集群, 熟悉Hadoop生态系统的相关产品, 熟悉HDFS分布式文件系统, Sqoop数据库ETL工具, 
Flume日志收集, MapReduce分布式计算框架, 熟练掌握Sql/Hql的编写, Zookeeper分布式协作, Yarn资源管理器
, Hive数据仓库, HBase实时协作数据库, 熟悉Spark内存计算, 熟悉MySQL/ORACLE数据库安装、管理、调优、备份
、容灾、安全等技能一般,熟悉MapReduce编程。熟悉Linux操作系统, 熟悉shell, 熟练运用SecureCRT, Xshell
, MyEclipse, FileZilla Client等应用软件。




比如你面离线或者实时, 简历中就突出上面对应的技术栈, 
以及对应技术栈的性能优化, 对应技术栈的项目经验。
比如面数仓开发技术栈写熟练掌握Hive的函数相关函数的使用, 
熟练进行业务开发；熟练大数据数仓建模；熟练掌握Hive常见异常分析, 
性能优化, 数据治理, 熟练掌握spark用于业务分析等。以及对应的技术栈的项目或数据分析经历。





数仓是通过分析客户信息, 账户信息, 企业关系, 事 件信息, 汇总数据, 清算数据, 
渠道信息为银行及金融机构的业务营销, 业 务推广, 费率调整, 优惠策略, 
为公司运营, 销售, 手续费, 风险控制, 为 企业商品营销, 资金调整提供数据报表、
风险评估、决策方向的业务系统。

 app 前端埋点数据通过 Flume 采集、
其他子系统业务数据库中的数据通过 logstash 采集, 银行前置机数据通过 
canal 采集后, 发送到 kafka 的不同 Topic之中, 使用Flume落地到HDFS, 
通过HIVE映射外部表, 配合spark sql 将数据清洗分层。该系统数据仓库分为四层。
ods 层为原始数据, dwd 层为清洗之后的数据, dws 层将数据轻度聚合, 
ads 层是分析完成的数据,  抽取到关系型数据库提供给页面展示。
系统中数据基于维度建模, 主要使用 星型模型, 主要包括用户, 商户, 
银行, 清算机构, 收单机构, 签约机构等 实体表, 区域, 时间, 交易类型, 
交易状态, 支付方式, 审计流程等维度 表, 代发, 代扣, 冲正, 
撤销明细等事务型事实表, 还有订单, 退款, 交 易, 审计等周期性事务表。
此项目中我参与过用户时间维度, 渠道维度, 区 域维度等交易量, 交易额, 
区域热门渠道交易排行, 区域漏斗分析, 退货,  异常交易占比等指标。



项目名称 : 中和农信大数据平台
技术架构 : Hadoop+Flume+Sqoop+Spark+Hive+Mysql+Tableau
项目描述 : 随着公司的发展, 数据规模扩大, 业务系统增多, 以及方便公司的决策, 需要建立大数据体系的数据仓库, 对公司的数据进行汇总, 为
各个部门提供统一规范的数据出口, 加强分析管理与决策

项目流程 : 
        维度建模 : 采用星型模型, 划分线上、贷后管理、保险等业务域, 划分客户、员工、财务、授信、策略、放款、收款、支用等主题域
        数据采集 : 使用Sqoop采集各个业务系统的数据, 通过Flume采集APP前端埋点数据, 落地到HDFS
        数据分层 : 使用Hive SQL对数据进行清洗加工, 将数据分为六层, ODS层--原始数据, 与业务数据库保持一致, 
                  DWD层--明细数据, DIM层--维度数据, DWI层--明细宽表, DWA层--汇总数据, RPT层--应用数据
        数据应用 : 取部分常用DWI明细宽表数据同步到Table Server上(如放款宽表、收款宽表、余额宽表及分支、区域、员工和客户详情表), 发布数据源, 方便产出报告及分支区域业务分析人员自主分析和获取明细数据, 
                   将RPT层数据(公司及各区域放款、收款、风险金额、Par1、Par30、账龄等)同步到Mysql上, 固定到运营平台展示
       
个人职责 : 参与数据仓库建模分层, 数据清洗加工, 报表产出等







项目名称 : 风险预警指标体系
技术架构 : Hadoop+Flume+Sqoop+Spark+Hive+Mysql
项目描述 : 由于公司的线下业务形式为客户经理放款, 贷款的质量与风险和客户经理息息相关,  通过建立客户经理所放贷款的相应指标体系, 建立评分模型, 
对客户经理进行预警与警报, 对公司的基层管理提供数据支持, 降低风险

项目流程 : 
        建立指标体系 : 根据业务情况, 将指标划分客户风险、产品风险、担保人风险、操作风险、逾期风险、征信风险、监测风险七大板块
        数据收集阶段 : 从数仓dwd层获取明细数据, 同步部分所需客户经理BAPP操作日志数据
        建立模型阶段 : 数据使用Hive SQL进行计算, 标准化处理后建立客户经理评分模型,  根据评分筛选预警以及警报的客户经理
        结果应用阶段 : 将存储在Hdfs上的结果表、评分表和明细用Sqoop同步到业务数据库(Mysql)上, 放到公司运营平台上展示, 并要求基层管理人员对预警与警报
                      客户经理进行调查, 将原因和处理结果进行反馈
个人职责 : 1、参与指标体系的构建, 
           2、数据的采集与计算, 
           3、模型的建立和任务的调度



项目名称 : 内审报表系统
技术架构 : Hadoop+Flume+Sqoop+Spark+Hive+Mysql
项目描述 : 为了更好的控制业务风险, 纠察公司员工的违规、违纪、违法等行为, 内审部门需对各个区域与分支定期查检, 此系统提供客户经理及分支
           名下客户的详细信息、贷款的流程信息、客户经理本人的操作记录、客户经理贷后等相关报表

项目流程 : 
         分析业务需求, 划分四个模块
             客户基本信息--客户住址、工作等是否真实, 有无照片套用等
             贷款信息及流程--是否进行贷审会、是否过桥、垒户
             客户还款记录--是否真实逾期、是否多人同账号还款
             客户经理操作行为--修改客户电话号码行为、虚拟账户余额及是否及时上划等
        使用Hive SQL生成报表, 并将表及明细数据同步到业务数据库, 开放给内审部门
个人职责 : 按照需求进行报表开发并展示

 



该系统是一个交互式用户行为分析系统。系统的主要用户为公司内部的PM和运营人员, 用户根据自己的需求去
分析某一 类客户的流量数据。根据分析结果, PM可以优化产品设计, 运营人员可以为自己的运营工作提供数据
支持。用户在系统界面中选择某个分析功能对应的菜单, 并进入对应的任务创建界面, 然后选择筛选条件和任务
参数, 并提交任务。在接收到用户提交的任务之后, 根据任务类型选择其对应的Spark作业, 启动一条子线
程来执行Spark-submit命令以提交Spark作业。Spark作业运行在Yarn集群上, 并针对Hdfs中的海量
数据使用SparkSQL进行计算, 最终将计算结果写入Hdfs中。另外还集成Flume, Kafka和Spark,
利用SparkStreaming, 进行实时分析。用户通过系统界面查看任务分析结果, 将结果返回给界面进行展现。

项目职责 :负责项目的集群搭建部署, 需求分析, 参与库表的设计及代码的编写和测试优化工作
开发步骤 : 数据收集阶段：Flume采集供应商接口上的数据,实现flume自定义拦截器,满足业务需求。
Kafka拉取flume上的数据传给sparkStreaming进行数据处理（实时ETL的程序）, ETL处理的数据保存到Hdfs中。
数据处理阶段：SparkCore处理Hdfs上的数据, 通过RDD进行离线处理和Dstream准实时处理之后保存到Hdfs中。
数据展示阶段：通过JDBC连接Hdfs处理好的数据, 进行图表展示。
功能模块 : 用户访问会话（session）分析模块, 页面跳出率统计分析模块, 
各区域热门商品统计分析模块, 广告点击流量实时统计分析模块








项目名称 : 每纷运营分析系统
技术架构 : Maxcompute+DataWorks+PyOdps+Quick BI
项目描述 : 项目主要功能为对APP用户的行为数据进行分析, 把产生的日志进行分析, 如pv、uv、商品关注率及地域维度等指标进行分析, 
并将分析结果提供给营销部门（营销部门提出需求）, 做业务开展情况分析, 为营销提供进一步参考。

项目流程 : 
        1, 数据采集：使用Flume将日志数据导入到DataHub, 并推送到Maxcompute上
        2, 数据计算：读取工作空间里的数据, 直接使用Maxcompute SQL和PyOdps对数据进行多维关联, 清洗, 过滤, 回填, 汇总
        3, 数据入库：使用套件数据同步完成对ETL处理后的数据的入库
        4, 数据分析：使用SQL完成主要的查询、统计等功能, 并分析, 例如商品的浏览及购买topN排序等
        5、结果展示：关联Quick BI与工作空间, 根据需求展示相应的图表分析结果
个人职责 : 
        SQL编写、报表完成及Quick BI数据门户、仪表板展示




